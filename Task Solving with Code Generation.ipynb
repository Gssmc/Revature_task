{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pyautogen/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pyautogen/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pyautogen-0.2.16-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting diskcache (from pyautogen)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen)\n",
      "  Using cached docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting flaml (from pyautogen)\n",
      "  Using cached FLAML-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.3 (from pyautogen)\n",
      "  Using cached openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pydantic!=2.6.0,<3,>=1.10 (from pyautogen)\n",
      "  Using cached pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "Collecting python-dotenv (from pyautogen)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen)\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai>=1.3->pyautogen)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=1.3->pyautogen)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai>=1.3->pyautogen)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=2.6.0,<3,>=1.10->pyautogen)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=2.6.0,<3,>=1.10->pyautogen)\n",
      "  Using cached pydantic_core-2.16.3-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=14.0 in d:\\gokul_sivakumar\\autogen\\.env\\lib\\site-packages (from docker->pyautogen) (23.2)\n",
      "Collecting requests>=2.26.0 (from docker->pyautogen)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3>=1.26.0 (from docker->pyautogen)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pywin32>=304 in d:\\gokul_sivakumar\\autogen\\.env\\lib\\site-packages (from docker->pyautogen) (306)\n",
      "Collecting NumPy>=1.17.0rc1 (from flaml->pyautogen)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->pyautogen)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=1.3->pyautogen)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->docker->pyautogen)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in d:\\gokul_sivakumar\\autogen\\.env\\lib\\site-packages (from tqdm>4->openai>=1.3->pyautogen) (0.4.6)\n",
      "Using cached pyautogen-0.2.16-py3-none-any.whl (207 kB)\n",
      "Using cached openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "Using cached pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "Using cached pydantic_core-2.16.3-cp311-none-win_amd64.whl (1.9 MB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "Using cached FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, termcolor, sniffio, regex, python-dotenv, NumPy, idna, h11, distro, diskcache, charset-normalizer, certifi, annotated-types, requests, pydantic-core, httpcore, flaml, anyio, tiktoken, pydantic, httpx, docker, openai, pyautogen\n",
      "Successfully installed NumPy-1.26.4 annotated-types-0.6.0 anyio-4.3.0 certifi-2024.2.2 charset-normalizer-3.3.2 diskcache-5.6.3 distro-1.9.0 docker-7.0.0 flaml-2.1.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 idna-3.6 openai-1.13.3 pyautogen-0.2.16 pydantic-2.6.3 pydantic-core-2.16.3 python-dotenv-1.0.1 regex-2023.12.25 requests-2.31.0 sniffio-1.3.1 termcolor-2.4.0 tiktoken-0.6.0 tqdm-4.66.2 typing-extensions-4.10.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST.json\",\n",
    "    filter_dict={\n",
    "    \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: what is 2^24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "what is 2 to the power of 24\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To calculate 2 to the power of 24, you can use the following Python code:\n",
      "\n",
      "```python\n",
      "# Calculate 2 to the power of 24\n",
      "result = 2 ** 24\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Please execute the Python code provided to calculate 2 to the power of 24.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "16777216\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The result of 2 to the power of 24 is 16,777,216.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config_list =[\n",
    "    {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"api_key\": \"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\"\n",
    "    }\n",
    "  \n",
    "]\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    },\n",
    ")\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"what is 2 to the power of 24\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: [{'content': 'what is 2 to the power of 24', 'role': 'assistant'}, {'content': 'To calculate 2 to the power of 24, you can use the following Python code:\\n\\n```python\\n# Calculate 2 to the power of 24\\nresult = 2 ** 24\\nprint(result)\\n```\\n\\nPlease execute the Python code provided to calculate 2 to the power of 24.', 'role': 'user'}, {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\n16777216\\n', 'role': 'assistant'}, {'content': 'The result of 2 to the power of 24 is 16,777,216.\\n\\nTERMINATE', 'role': 'user'}]\n",
      "Summary: The result of 2 to the power of 24 is 16,777,216.\n",
      "Cost info: ({'total_cost': 0, 'gpt-3.5-turbo-0125': {'cost': 0, 'prompt_tokens': 1203, 'completion_tokens': 104, 'total_tokens': 1307}}, {'total_cost': 0, 'gpt-3.5-turbo-0125': {'cost': 0, 'prompt_tokens': 1203, 'completion_tokens': 104, 'total_tokens': 1307}})\n"
     ]
    }
   ],
   "source": [
    "print(\"Chat history:\", chat_res.chat_history)\n",
    "\n",
    "print(\"Summary:\", chat_res.summary)\n",
    "print(\"Cost info:\", chat_res.cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
